{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF_SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuchiChourasia/DATA-MINING-PROJECT-2019/blob/master/TF_IDF_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zoJPKXYPEabY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mounting to Google Colab**"
      ]
    },
    {
      "metadata": {
        "id": "CyIpr5YwZwGi",
        "colab_type": "code",
        "outputId": "10491efd-a7b7-4104-af6e-d1ab242c24a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7tFRgVKi_WVp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import All Required Packages**"
      ]
    },
    {
      "metadata": {
        "id": "RuNPM8hvZlrM",
        "colab_type": "code",
        "outputId": "291b155b-4df0-4ddd-f623-757eaab682c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# used for encoding\n",
        "import chardet\n",
        "# natural language toolkit\n",
        "import nltk\n",
        "# wordnet is the database of English language\n",
        "nltk.download('wordnet')\n",
        "# stopwords for removing it from review\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "##from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# ngrams used for bigram input\n",
        "from nltk.util import ngrams\n",
        "import string\n",
        "import math\n",
        "# importing Linear SVM Classifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# evaluation measures\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jUAERBpsZlrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#universal variable declaration\n",
        "dataset = []\n",
        "\n",
        "# variable declaration for only review as the feature input\n",
        "trainData = []\n",
        "testData = []\n",
        "trainData_label = []\n",
        "testData_label = []\n",
        "\n",
        "# variable declaration for All the feature input\n",
        "trainDataAll = []\n",
        "testDataAll = []\n",
        "trainData_labelAll = []\n",
        "testData_labelAll = []\n",
        "\n",
        "# for removal of punctuation words\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "\n",
        "# Label lists\n",
        "label = []\n",
        "labelAll = []\n",
        "\n",
        "tfs = {}      # term frequency\n",
        "tfidf = {}    # term frequency Inverse document frequency\n",
        "\n",
        "tfs_test = {}    # term frequency for test data\n",
        "tfidf_test = {}  # term frequency Inverse document frequency for test data\n",
        "\n",
        "featureDict = {}         # A global dictionary of features\n",
        "featureDict_test = {}    # A global dictionary of All features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-NtngKrxwV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMPORT DATASET  **"
      ]
    },
    {
      "metadata": {
        "id": "Aub6OQ7EZlra",
        "colab_type": "code",
        "outputId": "8750cd10-bb09-4b84-fa5d-7418c9f99c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/amazon_data.csv\", encoding='Latin-1')\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Replace the Labels with FAKE and REAL\n",
        "df = df.replace({'LABEL': {'__label1__':'FAKE' , '__label2__':'REAL'}})\n",
        "\n",
        "display(df.head())\n",
        "\n",
        "# label list for Review only\n",
        "label=df['LABEL'].tolist()\n",
        "\n",
        "# label list for all as feature\n",
        "labelAll=df['LABEL'].tolist()\n",
        "\n",
        "# Drop the LABEL\n",
        "df=df.drop('LABEL',axis=1)\n",
        "row,col = df.shape\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>LABEL</th>\n",
              "      <th>RATING</th>\n",
              "      <th>VERIFIED_PURCHASE</th>\n",
              "      <th>PRODUCT_CATEGORY</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>PRODUCT_TITLE</th>\n",
              "      <th>REVIEW_TITLE</th>\n",
              "      <th>REVIEW_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10193</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>5</td>\n",
              "      <td>N</td>\n",
              "      <td>Video Games</td>\n",
              "      <td>B00D96BJSO</td>\n",
              "      <td>Turtle Beach - Ear Force XO Seven Premium Gami...</td>\n",
              "      <td>Great headset! READ THIS BEFORE YOU BUY PLEASE...</td>\n",
              "      <td>I got this headset as a replacement for a diff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16153</td>\n",
              "      <td>REAL</td>\n",
              "      <td>5</td>\n",
              "      <td>Y</td>\n",
              "      <td>Luggage</td>\n",
              "      <td>B007ML9B5A</td>\n",
              "      <td>Ricardo Beverly Hills Luggage Crystal City 20 ...</td>\n",
              "      <td>Perfect carry-on</td>\n",
              "      <td>Love this luggage. Spins, expands and great co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13541</td>\n",
              "      <td>REAL</td>\n",
              "      <td>5</td>\n",
              "      <td>Y</td>\n",
              "      <td>Toys</td>\n",
              "      <td>B00000IV35</td>\n",
              "      <td>Five Crowns</td>\n",
              "      <td>Great game</td>\n",
              "      <td>I bought this as a family game for both kids a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11901</td>\n",
              "      <td>REAL</td>\n",
              "      <td>5</td>\n",
              "      <td>N</td>\n",
              "      <td>Musical Instruments</td>\n",
              "      <td>B004ZIP22Y</td>\n",
              "      <td>SKB 3I-1711-XLX iSeries Injection Molded Case ...</td>\n",
              "      <td>Best case on the market bar none.</td>\n",
              "      <td>I bought this about two years ago. 1. The oute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10615</td>\n",
              "      <td>REAL</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>Toys</td>\n",
              "      <td>B0085MRBEI</td>\n",
              "      <td>Monster High Create-A-Monster Gargoyle-Vampire...</td>\n",
              "      <td>Um, you really want to buy this?</td>\n",
              "      <td>When i got it, i thought YES! but i should of ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DOC_ID LABEL  RATING VERIFIED_PURCHASE     PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
              "0   10193  FAKE       5                 N          Video Games  B00D96BJSO   \n",
              "1   16153  REAL       5                 Y              Luggage  B007ML9B5A   \n",
              "2   13541  REAL       5                 Y                 Toys  B00000IV35   \n",
              "3   11901  REAL       5                 N  Musical Instruments  B004ZIP22Y   \n",
              "4   10615  REAL       3                 N                 Toys  B0085MRBEI   \n",
              "\n",
              "                                       PRODUCT_TITLE  \\\n",
              "0  Turtle Beach - Ear Force XO Seven Premium Gami...   \n",
              "1  Ricardo Beverly Hills Luggage Crystal City 20 ...   \n",
              "2                                        Five Crowns   \n",
              "3  SKB 3I-1711-XLX iSeries Injection Molded Case ...   \n",
              "4  Monster High Create-A-Monster Gargoyle-Vampire...   \n",
              "\n",
              "                                        REVIEW_TITLE  \\\n",
              "0  Great headset! READ THIS BEFORE YOU BUY PLEASE...   \n",
              "1                                   Perfect carry-on   \n",
              "2                                         Great game   \n",
              "3                  Best case on the market bar none.   \n",
              "4                   Um, you really want to buy this?   \n",
              "\n",
              "                                         REVIEW_TEXT  \n",
              "0  I got this headset as a replacement for a diff...  \n",
              "1  Love this luggage. Spins, expands and great co...  \n",
              "2  I bought this as a family game for both kids a...  \n",
              "3  I bought this about two years ago. 1. The oute...  \n",
              "4  When i got it, i thought YES! but i should of ...  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BYqe3bM2_zue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**PARSE DATASET AND EXTRACT ESSENTIAL FEATURES FROM IT**\n"
      ]
    },
    {
      "metadata": {
        "id": "cYh6I9qLC_Ad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Function for Parsing Only Review Text**"
      ]
    },
    {
      "metadata": {
        "id": "WuX-NaUwZlrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse(review):\n",
        "    return (review[7])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5vusP1gE_6M8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Function for Parsing Rating, Verified Purchase, Product Category, Review Title and Review Text **"
      ]
    },
    {
      "metadata": {
        "id": "uHzedBbhUAyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parseAll(review):\n",
        "  return (review[1],review[2],review[3],review[6],review[7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xu34IfTHAvw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TEXT PREPROCESSING**"
      ]
    },
    {
      "metadata": {
        "id": "VE87otXIZlrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens=[]\n",
        "    lemmatized_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.translate(table)           # translation of text is done here punctuations are replaced with none\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in stop_words:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
        "            \n",
        "    \n",
        "        \n",
        "        #filtered_tokens = [' '.join(l) for l in nltk.bigrams(lemmatized_tokens)] + lemmatized_tokens\n",
        "                                          #nltk bigrams will make pair of word occur together mrore frequently\n",
        "                                          #here ' '.join(l) means it will join the words from the list (here it is l of iterable) with ' ' delimeter in between. \n",
        "    return lemmatized_tokens #lemmatized_tokens  #filtered_tokens\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lD20pORA_3e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**COUNT OF TOKENS IN THE TEXT**"
      ]
    },
    {
      "metadata": {
        "id": "haVNFQhFBCOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bow(Text):\n",
        "  \n",
        "    bowCount = len(Text)\n",
        "    \n",
        "    return bowCount"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKadI7RGBKto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION COMPUTING TERM FREQUENCY TF FOR EACH DOCUMENT**"
      ]
    },
    {
      "metadata": {
        "id": "XxFnw2QUZlsh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def computeTF(localDict, bowCount):\n",
        "    \n",
        "    tfDict = {}\n",
        "    for word, count in localDict.items():\n",
        "        tfDict[word] = count/float(bowCount)\n",
        "    \n",
        "    return tfDict\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUWGgx4lBRGM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION FOR COMPUTING THE INVERSE-DOCUMENT FREQUENCY IDF WITH RESPECT TO WHOLE DATA WHICH IS PASSED**"
      ]
    },
    {
      "metadata": {
        "id": "mrcX27lXZlrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def computeIDF(docList):\n",
        "   \n",
        "    idfDict = {}\n",
        "    #print(docList)\n",
        "    N = row ## row\n",
        "    #display(N)\n",
        "    \n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "      for key, value in doc.items():\n",
        "        \n",
        "        if value > 0:\n",
        "          idfDict[key] += 1\n",
        "              #print(idfDict)\n",
        "   \n",
        "    for word, val in idfDict.items():\n",
        "      idfDict[word] = math.log10(N / float(val))\n",
        "   \n",
        "    return idfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaFWSHmjBxYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION FOR COMPUTING THE TERM FREQUENCY INVERSE-DOCUMENT FREQUENCY TF-IDF FOR EACH DOCUMENT**"
      ]
    },
    {
      "metadata": {
        "id": "XOy3FsKKdg01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FOR TRAINING DATASET\n",
        "\n",
        "def computeTFIDF(tfs, idfs): \n",
        "    \n",
        "  for key,val in tfs.items():\n",
        "    t={}\n",
        "    for k,v in val.items():\n",
        "       t[k]=v*idfs[k]\n",
        "    tfidf[key] = t\n",
        "     \n",
        "  return tfidf\n",
        "\n",
        "# FOR TEST DATASET\n",
        "  \n",
        "def computeTFIDF_test(tfs_test, idfs_t): \n",
        "  \n",
        "  for key,val in tfs_test.items():\n",
        "    t1={}\n",
        "    for k,v in val.items():\n",
        "       t1[k]=v*idfs_t[k]\n",
        "    tfidf_test[key] = t1\n",
        "    \n",
        "  return tfidf_test\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7mOFCBfCa2x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION FOR CONVERTING IN TO VECTOR**"
      ]
    },
    {
      "metadata": {
        "id": "OwaRcuQbZlsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FOR TRAINING DATASET\n",
        "\n",
        "def toFeatureVector(tokens):\n",
        "    localDict = {}\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] += 1\n",
        "   \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] += 1\n",
        "        \n",
        "    return localDict\n",
        "  \n",
        "# FOR TEST DATASET\n",
        "\n",
        "def toFeatureVector_test(tokens1):\n",
        "    localDict1 = {}\n",
        "    for token1 in tokens1:\n",
        "        if token1 not in featureDict_test:\n",
        "            featureDict_test[token1] = 1\n",
        "        else:\n",
        "            featureDict_test[token1] += 1\n",
        "   \n",
        "        if token1 not in localDict1:\n",
        "            localDict1[token1] = 1\n",
        "        else:\n",
        "            localDict1[token1] += 1\n",
        "    \n",
        "    return localDict1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9wrJMPM1THEi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION CALLED FOR  COMPUTING TF-IDF VLAUE FOR DOCUMENTS**"
      ]
    },
    {
      "metadata": {
        "id": "oom-lKf3ZlsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FOR TRAINING DATASET\n",
        "\n",
        "def tfidf_compute(dataset1):\n",
        "  length=len(dataset1)\n",
        "  for i in range(length):\n",
        "      Text = parse(dataset1[i])\n",
        "      temp={}\n",
        "      temp={i:computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))}\n",
        "      tfs.update(temp)\n",
        "\n",
        "  idfs = computeIDF([featureDict])\n",
        "  tfidfs = computeTFIDF(tfs, idfs)\n",
        "  return tfidfs\n",
        "\n",
        "# FOR TEST DATASET\n",
        "\n",
        "def tfidf_compute_test(dataset2):\n",
        "  length1=len(dataset2)\n",
        "  for j in range(length1):\n",
        "      Text1 = parse(dataset2[j])\n",
        "      temp1={}\n",
        "      temp1={j:computeTF(toFeatureVector_test(preprocess(Text1)),bow(preprocess(Text1)))}\n",
        "      tfs_test.update(temp1)\n",
        "\n",
        "  idfs_t = computeIDF([featureDict_test])\n",
        "  tfidfs_test = computeTFIDF_test(tfs_test, idfs_t)\n",
        "  return tfidfs_test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXAOOdds5644",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION CALLED FOR  COMPUTING TF-IDF VLAUE FOR DOCUMENTS WITH ALL TAKEN AS FEATURES**"
      ]
    },
    {
      "metadata": {
        "id": "XoXopEefbUGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FOR TRAINING DATASET\n",
        "\n",
        "def tfidf_computeAll(dataset1):\n",
        "  length=len(dataset1)\n",
        "  for i in range(length):\n",
        "      R,VP,PC,Title,Text = parseAll(dataset1[i])\n",
        "      temp={}\n",
        "      temp={i:computeTF(toFeatureVector(preprocess(f'{R} {VP} {PC} {Title} {Text}')),bow(preprocess(f'{R} {VP} {PC} {Title} {Text}')))}\n",
        "      tfs.update(temp)\n",
        "\n",
        "  idfs = computeIDF([featureDict])\n",
        "  tfidfs = computeTFIDF(tfs, idfs)\n",
        "  return tfidfs\n",
        "\n",
        "# FOR TEST DATASET\n",
        "\n",
        "def tfidf_compute_testAll(dataset2):\n",
        "  length1=len(dataset2)\n",
        "  for j in range(length1):\n",
        "      R,VP,PC,Title,Text = parseAll(dataset2[j])\n",
        "      temp1={}\n",
        "      temp1={j:computeTF(toFeatureVector_test(preprocess(f'{R} {VP} {PC} {Title} {Text}')),bow(preprocess(f'{R} {VP} {PC} {Title} {Text}')))}\n",
        "      tfs_test.update(temp1)\n",
        "\n",
        "  idfs_t = computeIDF([featureDict_test])\n",
        "  tfidfs_test = computeTFIDF_test(tfs_test, idfs_t)\n",
        "  return tfidfs_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IoeHrm0K6Iml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TAKING INPUT FROM DATAFRAME TO LIST FORM**"
      ]
    },
    {
      "metadata": {
        "id": "cGCVA0e6ZlsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset=list(df.values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfZgztrF6PlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SPLITTING DATASET INTO DESIRED PERCENTAGE OF TRAIN AND TEST DATASET**"
      ]
    },
    {
      "metadata": {
        "id": "r6a9JJyNP1ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def split(percent):\n",
        "    dataSamples = len(dataset)\n",
        "    halfOfData = int(len(dataset)/2)\n",
        "    trainingSamples = int((percent*dataSamples)/2)\n",
        "    \n",
        "    # TRAIN data here\n",
        "  \n",
        "    x1=dataset[:trainingSamples] + dataset[halfOfData:halfOfData+trainingSamples]\n",
        "    # calling tfidf_compute\n",
        "    train_tfidf = tfidf_compute(x1)\n",
        "  \n",
        "    for m,n in train_tfidf.items():\n",
        "      # appending tfidf value (n) to the documents into trainData which is a list\n",
        "      trainData.append((n))\n",
        "    \n",
        "   \n",
        "    for x in label[:trainingSamples]+label[halfOfData:halfOfData+trainingSamples]:\n",
        "      # appending Labels of data in to another list \n",
        "      trainData_label.append(x)\n",
        "    \n",
        "    # TEST data here\n",
        "    \n",
        "    x2=dataset[trainingSamples:halfOfData] + dataset[halfOfData+trainingSamples:]\n",
        "    test_tfidf = tfidf_compute_test(x2)\n",
        "    \n",
        "   \n",
        "    for m_t,n_t in test_tfidf.items():\n",
        "      testData.append((n_t))\n",
        "    \n",
        "   \n",
        "    for y in label[trainingSamples:halfOfData] + label[halfOfData+trainingSamples:]:\n",
        "      testData_label.append(y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvmlQY2DJktV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SPLITTING DATASET IN TO TRAIN AND TEST DATASET FOR ALL THE FEATURES** "
      ]
    },
    {
      "metadata": {
        "id": "jw5tqDkr0fPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitAll(percent):\n",
        "  \n",
        "    dataSamplesAll = len(dataset)\n",
        "    halfOfDataAll = int(len(dataset)/2)\n",
        "    trainingSamplesAll = int((percent*dataSamplesAll)/2)\n",
        "    \n",
        "    # TRAIN Data here  \n",
        "  \n",
        "    x11 = dataset[:trainingSamplesAll] + dataset[halfOfDataAll:halfOfDataAll+trainingSamplesAll]\n",
        "    train_tfidf = tfidf_computeAll(x11)\n",
        "    for m11,n11 in train_tfidf.items():\n",
        "      trainDataAll.append((n11))\n",
        "    \n",
        "   \n",
        "    for x in labelAll[:trainingSamplesAll]+labelAll[halfOfDataAll:halfOfDataAll+trainingSamplesAll]:\n",
        "      trainData_labelAll.append(x)\n",
        "    \n",
        "    # TEST Data here\n",
        "    \n",
        "    x22 = dataset[trainingSamplesAll:halfOfDataAll] + dataset[halfOfDataAll+trainingSamplesAll:]\n",
        "    test_tfidf = tfidf_compute_testAll(x22)\n",
        "    \n",
        "   \n",
        "    for m_t11,n_t11 in test_tfidf.items():\n",
        "      testDataAll.append((n_t11))\n",
        "    \n",
        "   \n",
        "    for y in labelAll[trainingSamplesAll:halfOfDataAll] + labelAll[halfOfDataAll+trainingSamplesAll:]:\n",
        "      testData_labelAll.append(y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OrJ8s93P9wds",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TRAINING CLASSIFIER FUNCTION USING LINEAR SVM**"
      ]
    },
    {
      "metadata": {
        "id": "Ncai1h1_Zlsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainClassifier(trainclassifier):\n",
        "  \n",
        "    print(\"Training Classifier\")\n",
        "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
        "    \n",
        "    return SklearnClassifier(pipeline).train(trainclassifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82MQNn_Y98n2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CROSS-VALIDATE FUNCTION **"
      ]
    },
    {
      "metadata": {
        "id": "wdpdtRg0Uyvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crossValidate(dataset_cv, folds):\n",
        "  \n",
        "    cv_results = []\n",
        "    \n",
        "    # Each fold's Size of the dataset\n",
        "    \n",
        "    foldSize = int(len(dataset_cv)/folds)\n",
        "    \n",
        "    for i in range(0,len(dataset_cv),foldSize):\n",
        "      \n",
        "        # Training classifier for each foldsize traing set\n",
        "        classifier = trainClassifier(dataset_cv[:i]+dataset_cv[foldSize+i:])\n",
        "        \n",
        "        # Predicting for the specified test set for the foldsize\n",
        "        y_pred = predictLabels(dataset_cv[i:i+foldSize],classifier)\n",
        "        \n",
        "        # Measuring accuracy for test set\n",
        "        a = accuracy_score(list(map(lambda d : d[1], dataset_cv[i:i+foldSize])), y_pred)\n",
        "        \n",
        "        # Measuring Precision ,Recall, F1-Score \n",
        "        (p,r,f,_) = precision_recall_fscore_support(list(map(lambda d : d[1], dataset_cv[i:i+foldSize])), y_pred, average ='macro')\n",
        "        cv_results.append((a,p,r,f))\n",
        "    cv_results = (np.mean(np.array(cv_results),axis=0))\n",
        "    return cv_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ioztFXM_lVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FUNCTION FOR PREDICTING LABELS FOR TEST SET**"
      ]
    },
    {
      "metadata": {
        "id": "4du4lDisZlry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictLabels(reviewSamples, classifier1):\n",
        "  \n",
        "    # in-line function to map featue to classifier\n",
        "    # Only keeping the features and not taking the labels to the classifier\n",
        "    # t[0]\n",
        "    \n",
        "    return classifier1.classify_many(map(lambda t: t[0], reviewSamples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEfPpvd3AOJf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SPLIT FUNCTION IS CALLED FOR ONLY REVIEW AS FEATURE**"
      ]
    },
    {
      "metadata": {
        "id": "8LLlO2TG9XbP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split(0.95)\n",
        "\n",
        "## Make sure to input same portion of data in both the split as some function and data are shared by both ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QIzDrBtJawL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TRAIN DATA AND TEST DATA **"
      ]
    },
    {
      "metadata": {
        "id": "N3KFSa6qZlst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# np.c_ is used to take both data and label as input together\n",
        "\n",
        "TRAIN_MAIN_DATA = np.c_[trainData,trainData_label]\n",
        "TEST_MAIN_DATA = np.c_[testData,testData_label]\n",
        "\n",
        "# converting train and test set in to list form as desired input to classifier is in list form\n",
        "\n",
        "TRAIN_MAIN_DATA = TRAIN_MAIN_DATA.tolist()\n",
        "TEST_MAIN_DATA = TEST_MAIN_DATA.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xkaqZeMAMlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CROSS-VALIDATE CALLED FOR ONLY REVIEW AS A FEATURE**"
      ]
    },
    {
      "metadata": {
        "id": "NNlxUNkPVBNJ",
        "colab_type": "code",
        "outputId": "197d4d3b-5b0d-48df-aaad-9325f16a0c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(TRAIN_MAIN_DATA, 10))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.67894737 0.67902339 0.67893423 0.67883726]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zyfEI83HAPjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TESTING ACCURACY FOR ONLY REVIEW AS FEATURES **"
      ]
    },
    {
      "metadata": {
        "id": "wCXQLAasshOc",
        "colab_type": "code",
        "outputId": "9a09f837-ed74-47f3-efad-7e9118d5e19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "## Test Accuracy\n",
        "classifier = trainClassifier(TRAIN_MAIN_DATA)\n",
        "predictions = predictLabels(TEST_MAIN_DATA, classifier)\n",
        "true_labels = list(map(lambda d: d[1], TEST_MAIN_DATA))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", r)\n",
        "print(\"f1-score: \", f1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "accuracy:  0.68\n",
            "Precision:  0.6798187159774702\n",
            "Recall:  0.6797456085900662\n",
            "f1-score:  0.6797722825120086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tu64hwCtAQ_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SPLIT FUNCTION CALLED FOR ALL AS FEATURE**"
      ]
    },
    {
      "metadata": {
        "id": "26o01LKv0faY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "splitAll(0.95)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmAALSyt9eHN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TRAIN AND TEST FOR ALL FEATURE **"
      ]
    },
    {
      "metadata": {
        "id": "UAjhdWNfFboo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "891ffa7b-b09c-49db-8a92-e12629553118"
      },
      "cell_type": "code",
      "source": [
        "print(len(trainData_labelAll))\n",
        "print(len(trainDataAll))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19950\n",
            "19950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zkY2roam1mrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# np.c_ is used to concatenate data and label as input together for classifier\n",
        "\n",
        "\n",
        "TRAIN_MAIN_DATA_All=np.c_[trainDataAll,trainData_labelAll]\n",
        "TEST_MAIN_DATA_All=np.c_[testDataAll,testData_labelAll]\n",
        "\n",
        "# converting train and test set in to list form as desired input to classifier is in list form\n",
        "\n",
        "TRAIN_MAIN_DATA_All=TRAIN_MAIN_DATA_All.tolist()\n",
        "TEST_MAIN_DATA_All=TEST_MAIN_DATA_All.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChwsgKy1A92o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CROSS-VALIDATE CALLED FOR ALL AS FEATURE**"
      ]
    },
    {
      "metadata": {
        "id": "XVZV_jnH2-ta",
        "colab_type": "code",
        "outputId": "fd0b374e-ba88-4790-ecba-de8725bf97bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(TRAIN_MAIN_DATA_All, 10))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.82165414 0.82307186 0.82166007 0.82141071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "epZVVzQjBEqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TESTING ACCURACY FOR ALL FEATURES **"
      ]
    },
    {
      "metadata": {
        "id": "_1v65dJc6Nsb",
        "colab_type": "code",
        "outputId": "0c256120-131c-4392-d546-b57f6a079a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "## Test Accuracy\n",
        "classifier_All = trainClassifier(TRAIN_MAIN_DATA_All)\n",
        "predictions_All = predictLabels(TEST_MAIN_DATA_All, classifier_All)\n",
        "true_labels_All = list(map(lambda d: d[1], TEST_MAIN_DATA_All))\n",
        "a_All = accuracy_score(true_labels_All, predictions_All)\n",
        "p_All, r_All, f1_All, _ = precision_recall_fscore_support(true_labels_All, predictions_All, average='macro')\n",
        "print(\"accuracy: \", a_All)\n",
        "print(\"Precision: \", p_All)\n",
        "print(\"Recall: \", r_All)\n",
        "print(\"f1-score: \", f1_All)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "accuracy:  0.8285714285714286\n",
            "Precision:  0.8345993128424181\n",
            "Recall:  0.8270007731930695\n",
            "f1-score:  0.8272451729193555\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}